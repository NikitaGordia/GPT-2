# GPT-2 model configuration

name: gpt2-xl

# Model architecture parameters
block_size: 1024  # Maximum sequence length
vocab_size: 50257 # GPT-2 vocabulary size
n_layer: 48       # Number of transformer layers
n_head: 25        # Number of attention heads
n_embd: 1600      # Embedding dimension

_target_: gpt.model.GPTConfig