# GPT-2 model configuration

name: gpt2

# Model architecture parameters
block_size: 1024  # Maximum sequence length
vocab_size: 50257  # GPT-2 vocabulary size
n_layer: 12       # Number of transformer layers
n_head: 12        # Number of attention heads
n_embd: 768       # Embedding dimension

_target_: gpt.model.GPTConfig
